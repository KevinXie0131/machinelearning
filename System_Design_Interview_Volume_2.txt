+++Chapter 17: Proximity Service

search radius / expand the search
Functional requirements / Non-functional requirements
latitude and longitude pair

LBS: location based service
user privacy / comply with data privacy laws
the spike in traffic during peak hours

API design / Data model (schema design) / High-level design / deep dive into
search results are paginated
reviews / star rating
a new endpoint call to fetch the detailed information

Google Places API: a service that accepts HTTP requests for location data through a variety of methods
read/write ratio / read volume / write volume / infrequent operations

Geospatial indexing is a technique used in databases to efficiently store and retrieve data based on their geographic location
Geohash is a unique identifier of a specific region on the Earth. For a given location on earth, the Geohash algorithm converts its latitude and longitude into a string.

Database cluster can be used for primary-secondary setup
due to replication delay
off-peak hours
different regions and availability zones

geospatial database such as GeoHash in Redis or PostgreSQL with PostGIS extension
Redis geospatial indexes let you store coordinates and search for them. This data structure is useful for finding nearby points within a given radius or bounding box.

Option 1: Two-dimensional search: not efficient
Option 2: Evenly divided grid: the distribution of business is not even / need more granular grids for dense areas and large grids in sparse areas
Option 3: Geohash: reduce two-dimensional longitude and latitude data into one-dimensional string ofa letters and digits / has 12 precisions (also called levels)
                   only interested in geohashes with lengths between 4 and 6 / edge cases with how the geohash boundary is handled
                   the longer a shared prefix is between two geohashes, the closer they are
                   a common solution is to fetch not only within the current grid but also from its neighbors

Option 4: Quadtree: (a tree data structure in which each internal node has exactly four children)
                   In memory data structure, not database solution. It doesn't take too much memory and easily fit in one server
                   incrementally rebuild the quadtree, a small subset of servers at a time, across the entire cluster
                   roll out a new release / Blue/green deployment
                   mitigated by setting up a business agreement / update the cache using a nightly job
                   update it on the fly

Option 4: Google S2: a library for spherical geometry / in-memory solution / maps a sphere to 1D index based on Hilbert curve

compound key of (geohash, business_id)
have a series of replicas to help with the read load
a key-value store like Redis
no locking mechanism is needed
fetch fully hydrated business information

------------------------------------------------------------------------------------------------------------------------
+++Chapter 18: Nearby Friends

reasonable assumption / constraints and assumptions
inactive friends
privacy and data laws
occasional data loss
location refresh interval
load more upon request

stateful and bi-directional WebSocket services: each client maintains a persistent WebSocket connection to one of these servers
spread out load evenly
Restful API servers: stateless HTTP servers

Redis location cache: store the most recent location data for active user / Time to Live (TTL)
Redis pub/sub server: lightweight message bus / channels, also called topics
broadcast the update to all the subscribers
Redis provides super-fast read/write operations. It supports TTL used to auto-purge inactive users from the cache.
Redis servers are easily shard based on user id
Redis pub/sub server is used as a routing layer to direct messages

can be horizontal scaled / shard by user id
at a lower scale / increase the scale
inactivity timeout period
minimize downtime
conservative estimation

service discovery: Zookeeper & etcd (A distributed, reliable key-value store for the most critical data of a distributed system)
service discovery is a small key-value store to hold configuration data

consistent hashing / hash ring
over-provisioned / operational overhead and risks
performance hotspots
the incremental load should not overwhelm any single one

nearby random person: a pool of pub/sub by geohash

WebSocket: real-time communication between clients and the servers
Redis: fast read and write of location data
Redis Pub/Sub: routing layer to direct location updates from one user to all the online friends
Redis Pub/Sub implements a real-time messaging system, where publishers, publish to a channel/topic and several clients can subscribe to that channel/topic.

------------------------------------------------------------------------------------------------------------------------
+++Chapter 19: Google Maps

OpenStreetMap
Geocoding is the process of converting addresses to geographic coordinates
Geohashing is an encoding system that encodes a geographic area into a short string of letters and digits

Dijkstra's or A* pathfinding algorithms
nodes(intersections) and edges(roads)

rough estimation / server throughput / GPS are batched
analyze user behavior to enable personalization / leverage the location data in near real-time
sent in batch to the server at a lower frequency

communication protocol: HTTP with the keep-alive option
CDN fetches a copy from the origin server, caches it locally and returns it to the user
CDN returns a cached copy without contacting the origin server

keep mobile data usage low / client-side caching
a fast lookup mechanism

cloud storage, Amazon S3
prioritize availability over consistency
partition kay

Kafka: message queue. A unified low-latency, high-throughput data streaming platform designed for real-time data feeds
------------------------------------------------------------------------------------------------------------------------
+++Chapter 20: Distributed Message Queue

systems are broken up into small and independent building blocks with well-defined interfaces between them
Message queues provide communication and coordination for those building blocks

Benefits of message queues:
    - Decoupling: eliminate tight coupling
    - Improved scalability: more consumers are added to handle the increased traffic during peak hours
    - Increased availability: if part of system goes offline, the other continue to interact with queue
    - Better performance: make asynchronous communication easy. Don't need to wait for each other (add to a queue without waiting for response / consume messages whenever they are available)

Apache Kafka (distributed event store and stream-processing platform) / Apache RocketMQ / RabbitMQ / Apache Pulsar / Apache ActiveMQ / ZeroMQ

Kafka and Pulsar are not message queues, but event streaming platform
performant enough

design a distributed message queue with additional features, such as long data retention, repeated consumption of messages, etc., which are typically only available on event streaming platforms
basic functionality of message queue: producers send messages to a queue, and consumers consume messages from it
other considerations include performance, message delivery semantics (at-most-once, at-least-once, exactly-once), data detention, etc.

Apache Kafka Message Delivery Semantics:
    - At-Most-Once: a message is delivered either one time only or not at all / suitable for monitoring metrics / ack = 0
    - At-Least-Once: a message can be delivered one or more times, but will never be lost. / result in duplicates / suitable for client side which has deduplication / ack = 1 or ack = all
    - Exactly-Once: a  message will always be delivered only one time. / suitable for financial related user cases (payment, accounting and trading) / downstream service doesn't support idempotency

a traditional distributed message queue doesn't retain a message once it has ben successfully delivered to a consumer
a traditional distributed message queue doesn't guarantee delivery orders
a traditional distributed message queue provides on-disk overflow capacity

Messages are measured in the range of KBs
target throughput / end-to-end latency
support a sudden surge in message volume
several orders of magnitude smaller than

producer / consumer / produce / consume / subscribe
message queue is server, and producer/consumer are clients in server/client model

Messaging models:
   - Point-to-point: consumed by only one consumer / There is no data retention in this model
   - Publish-subscribe: implemented by topics

Topics: the categories used to organize messages / each topic has a unique name
Partitions (sharding): a small subset of the messages for a topic
Offset: the position of a message in the partition
Brokers: the servers which hold the partitions
Consumer group: the consumers form a Consumer group for a partition / a set of consumers consume messages from topics together

pull data from these partitions of message queue cluster
a constraint: a single partition can only be consumed by one consumer in the same group

Coordination service: Apache Zookeeper / etcd are used to elect a a controller
send message in batches/ persist messages in even larger batches

Data storage:
   - Option 1: database: database doesn't support read-heavy and write heavy access patterns at a large scale
   - Option 2: Write-ahead-log (WAL). WAL is just a plain file where new entries are appended to an append-only log
               WAL has a pure sequential read/write access pattern. The disk performance of sequential access is good

messages are in transit
cyclic redundancy check (CRC)
routing layer: all messages sent to the routing layer are routed to the correct broker
leader/follower replicas: fault-tolerance

Buffer
   - fewer network hops mean lower latency
   - Batching buffers messages in memory and sends out larger batches in a single request. This increases throughput
The size of batch is a trade-off between throughput and latency

Push vs Pull (most choose pull model)
    push model:
        pros: low latency
        cons: consumers could be overwhelmed
    pull model
        pros: consumers control the consumption rate / can simply scale out the consumers / more suitable for aggressive batch processing
        cons: If there is no message, keeping pulling is wasting resources.

consumer re-balancing / partition dispatch plan / heartbeat

State Storage stores:
   - The mapping between partitions and consumers
   - The last consumed offsets of consumer groups for reach partition
   access pattern for consumer state:
      - Frequent read/write operations but the volume is not high
      - Data is updated frequently and is rarely delete
      - Random read/write operations
      - Data consistency is important
   A key-value store like Zookeeper is a good choice

Metadata storage stores the configuration and properties of topics, including a number of partitions, retention period, and distribution of replicas
Zookeeper is a good choice for storing metadata (high consistency requirement)

Zookeeper offers a hierarchical kay-value store and used to provide a distributed configuration service, synchronization service, and naming registry
Zookeeper helps with the leader election of broker cluster

Replication is the solution to achieve high availability
replica distribution plan
In-sync replicas (ISR) / committed offset / fully caught up with the leader / lag time / trade-off between performance and durability
acknowledge setting:
    ACK = all: producer get acknowledge after all ISRs received message / the strongest message durability
    ACK = 1: producer get acknowledge once the leader persists message / The latency is improved by not waiting for data synchronization / occasional data loss is acceptable
    ACK = 0: producer keeps sending message without waiting for any acknowledge / lowest latency at the cost of potential data loss / good for collecting metrics or logging data since data volume is high but data loss is acceptable

data mirroring copies data across data centers
server crashes / failure recovery of the brokers / tolerate failure of the node
replicas should not be in the same node
decommissioned partition

Message filtering: consumer fetches the full set of messages and filters out unnecessary messages
                   attach a tag to metadata of each message and broker can filter message in that dimension

RocketMQ support delayed messages with specific levels of time precision
Scheduled message is similar with delayed message

Protocol: Advanced Message Queueing Protocol (AMQP) / Kafka Protocol
    - Cover all activities such as production, consumption and heartbeat
    - Effectively transport data with large volumes
    - Verify the integrity and correctness of the data
Retry consumption: send failed messages to dedicated retry topic
Historical data archive: time-based or capacity-based log retention mechanism / use HDFS or object storage to store historical data
------------------------------------------------------------------------------------------------------------------------
+++Chapter 21: Metrics Monitoring and Alerting System

Datadog / InfluxDB / New Relic / Nagios / Prometheus / Munin / Grafana / Graphite / OpenTSDB

in-house system / 1-year retention
reduce the resolution of the data for long-term storage / roll them up to 1-min resolution / raw form of data
distributed system tracing
Log monitoring: Elasticsearch, Logstash, Kibana (ELK) stack

Data collection
Data transmission
Data storage
Alerting: detect anomalies
Visualization

Data model: Metrics data recorded a a time series that contains a set of values with associated timestamps
the line protocol: common input format for monitoring software
collected at high frequency
read volume could be bursty / spiky

time-series database: OpenTSDB / MetricsDB / Timestream /  InfluxDB / Prometheus
in-memory cache / on-disk storage

feature of time-series database is efficient aggregation and analysis of a large amount of time-series data by labels, also known as tags in some databases

Metrics data / Metrics source
Metrics collector:
    Pull vs Push model:
        Pull model: pull metrics values from running applications periodically
                    Service Discovery (etcd, Zookeeper): Metrics collector needs to know the complete list of service endpoints to pull data from
                                                         configuration metadata of service endpoints from service discovery: pulling interval, IP addresses, timeout and retry parameters
                                                         metrics collector pulls metrics data via a pre-defined HTTP endpoint
                                                         metrics collector polls from endpoint changes periodically
                    A pool of metrics collector: One potential approach is to designate each collector to a range in a consistent hash ring
        Push model: collection agent installed on every server monitored
                    collection agent is a long-running software to collect metrics from the services running on the server and pushed those metrics periodically to the metrics collector
                    Aggregation: reduce the volume of data sent to the metrics collector


Scale metrics transmission (ingestion) pipeline
    metrics collector send metrics data to queueing system like Kafka, then consumers or streaming processing services
    such as Apache Storm, Flink, and Spark process the push data to the time-series database

Scale Kafka
    - Partition metrics data by metric names, so consumers can aggregate data by metrics names
    - Further partition metrics data with tags/labels

Aggregations:
    Collection agent: only support simple aggregation logic, like aggregate a counter
    Ingestion pipeline: stream processing engines like Flink / only calculated result is written to database / downside is that we no longer store raw data
    Query side: query speed might be slow / For a well-chosen time-series database, there is no need to add own caching

Data encoding and compression can significantly reduce the size of the data, like Double-delta Encoding
Down-sampling is the processing of converting high-resolution data to low-resolution to reduce overall disk usage
Cold storage is the storage of inactive data that is rarely used

Build vs Buy / available off-the-shelf alerting systems
Visualization system: Grafana
------------------------------------------------------------------------------------------------------------------------
+++Chapter 22: Ad Click Event Aggregation

One core benefit of online advertising is its measurably, as quantified by real-time data
digital advertising has a core process called Real-Time Bidding(RTB) / the speed of RTB is less than 1 second

Publisher -> Supply Side Platform -> Ad Exchange <- Demand Side Platform <- Advertiser

Data accuracy / click-through rate (CTR) / conversion rate (CVR) / targeted audience group
parameters should be configurable /  end-to-end latency should be a few minutes

Query API design: run query against the aggregation service
    API 1: Aggregate the number of clicks of ad_id in the lat M minutes
    API 2: Return top N most clicked ad_ids in the last M minutes

Data model:
   Raw data: Data is scattered on different application servers / backup data
   Aggregated (derived) data: add additional field call filed_id / Records are grouped by filed_id / active data

unbounded data stream / aggregated result
Asynchronous processing: adopt a message queue, Kafka to decouple producers and consumers / producers and consumers can scaled independently

the second message queue like Kafka to achieve end-to-end exactly-once semantics (atomic commit)

Aggregation service: MapReduce framework is a good solution to aggregate ad click events / Directed acyclic graph (DAG)
map > aggregate -> reduce -> top 10 aggregation

data filtering: star schema / the filtering fields are called dimensions

Streaming (near real-time system / Flink / infinite stream) vs Batching (offline system / MapReduce / bounded input with finite size)
lambda architecture: In addition to the batch layer and speed layers, Lambda architecture also includes a data serving layer for responding to user queries.
kappa architecture

Aggregation window / 4 types of window functions:
    - tumbling/fixed window: time is partitioned into same-length, non-overlapping chunks
    - hopping window: a windows slides across the data stream, according to a specific interval / get the top N most clicked ads during the last M minutes
    - sliding window
    - session window

data deduplication: malicious intent is handled by ad fraud/risk control components / server outage / upstream service hasn't received acknowledge, stream event might be sent again
A distributed transaction is a transaction that works across several nodes
To achieve exactly-once in Kafka, offset can be save in HDFS/S3 after acknowledge is back from downstream

during off-peak hours to minimize the impact / pre-allocate enough partitions in advance
topic physical sharding: Slicing data to different topics can help increase system throughput. With fewer consumers for a single topic,
                         the time to re-balance consumer groups is reduced. However, it introduces extra complexity and maintenance costs

How to increase the throughput of the aggregation service:
    Option 1: Allocate events with different ad_ids to different threads
    Option 2: Deploy aggregation service nodes on resource providers like Apache Hadoop YARN

Hotspot issue: mitigated by allocating more aggregation nodes to process popular ads

Fault-tolerant: save the 'System status' like upstream offset to a snapshot and recover from the latest saved status
Failover process: if one aggregation service node fails, we bring up a new node and recover data from the latest snapshot
Reconcile with / Reconciliation: compare different sets of data in order to ensure data integrity / a batch job
------------------------------------------------------------------------------------------------------------------------
+++Chapter 23: Hotel Reservation System

the scale of the system / overbooking / within the scope / out of the scope
average transaction per second (TPS)
QPD distribution / reach the final step / drop off the flow / walk backwards along the funnel

not technically challenging
used as idempotency key to prevent double booking
a few oder of magnitudes higher than

relational database works well with ready-heady and write less frequently workflows
NoSQL databases are generally optimized for writes and relational database works well with ready-heavy workflow

the relationship between different entities is stable
Reservation status: pending / paid / refunded / canceled / rejected

standard room / king-size room / queen-size room
design is modeled with microservice architecture / high-level design diagram

Public API Gateway: rate limiting / authentication / configured to direct | route requests to specific service based on endpoints
Internal APIs: authorized staff / further protected by VPN
inter-service communication often employs a modern and high-performance remote procedure call (RPC) framework like gRPC

inventory / composite primary key / a scheduled daily job to pre-populate the inventory data
set up database replication across multiple regions or availability zones
history can be achieved and moved to cold storage

Database sharding: hotel_id is a good sharding key. the data can be sharded by hash(hotel_id)%number_of_servers

concurrency issue:
    - the same user books multiple times:
        + client-side implementation: not very reliable
        + idempotent APIs: add an idempotency key in the reservation API request
                           use the idempotency kay (reservation_id) to avoid double reservation issue (unique constraint violation)
                           The unique reservation_id is generated by a globally unique ID generator
    - multiple users try to book the same room at the same time
        + locking mechanism: pessimistic locking (pessimistic concurrency control):
                                 > prevent simultaneous updates by placing a lock on a record as soon as one user starts to update it
                                 > other users have to wait until the first user has released the clock
                                 > Begin Transaction / Commit Transaction / Rollback Transaction
                                 > Pros: Easy to implement and avoid conflict by serializing updates
                                 > Cons: deadlocks may occur when multiple resources are locked / This approach is not stable / So do not recommend
                             optimistic locking (optimistic concurrency control):
                                 > version number (better option because server clock cannot be accurate over time)
                                      the next version number should exceed the current version number by 1 / Transaction aborts if the validation fails
                                      Pros: prevent application from editing stale data
                                             no need to lock database resource. up to application to handle logic
                                      Cons: performance is port when data contention is heavy
                                 > timestamp
                             database constraints: Add a Constraint of CHECK
                                                   Easy to implement and works well when data contention is minimal
                                                   database constraint cannot be version-controlled easily by code
                                                   not all databases support constraints

Database sharding: to split data into multiple databases so that each of them only contains a portion of data
Inventory Caching: Redis is a good choice because TTL and Least Recently Used (LRU) cache eviction policy help us male optimal use of memory / as a precaution / the source of true for the inventory data
The volume of read operations is an order of magnitude higher than write operations

How to maintain consistency between cache and database
    - propagated to the cache asynchronously
    - Change Data Capture (CDC) / Debezium
    - final inventory validation check / some else just booked the last room

Data consistency among services: In microservice architecture, each service has its own database. one logically atomic operation can span multiple services
                                 there is only one happy path, and many failure case could cause data inconsistency
    - Two-phase commit (2PC)
        guarantee atomic transaction commit across multiple nodes / blocking protocol / not performant
    - SAGA design pattern
        a way to manage data consistency across microservices in distributed transaction scenarios
        a sequence of local transactions
        consist of multiple steps and rely on eventual consistency
------------------------------------------------------------------------------------------------------------------------
+++Chapter 24: Distributed Email Service

anti-span anti-virus
SMTP (Simple Mail Transfer Protocol) and is a protocol for sending emails.
POP (Post Office Protocol Version 3) is a simple protocol that only allows downloading messages from your Inbox to your local computer.
IMAP (Internet Message Access Protocol) is much more advanced and allows the user to see all the folders on the mail server.
should not degrade with

MIME (Multipurpose Internet Mail Extensions): an Internet standard that extends the format of email messages to support text in character sets other than ASCII,
                                              as well as attachments of audio, video, images, and application programs

public-facing request/response services
stateful servers that maintain persistent connections
Establish WebSocket connection
inverted index to support fast full-text searches
Real-time servers are WebSocket servers that allow clients to received new emails in real-time

relational databases are optimized for small chunks of data entries / search queries over unstructured BLOB data type are not efficient / not good fits
single-digit of MB / create incremental backups

Partition key: distribute data across nodes evenly
Clustering key: sorting data within a partition
composite partition key <user_id, folder_id> is used
email_id's data type is TIMEUUID, and it is clustering key used to sort emails in chronological order

NoSQL database normally only supports queries on partition and cluster keys
one way to get around this limitation is to fetch the entire folder and perform filtering in application
This problem is commonly solved with de-normalization in NoSQL. denormalize into two tables, read_emails & unread_emails

Email threading based on the JWZ algorithm

Option 1: Elasticsearch
    - perform reindexing / reindexing can be done with offline jobs
    - Elasticsearch is a good option as it is easy to integrate and doesn't require significant engineering effort
    - Kafka is used to decouple services that trigger reindexing from services that actually perform reindexing
Option 2: Custom search solution
    - the main bottleneck of index server is usually disk I/O
    - the process of building index is write-heavy
    - Log-Structured Merge-Tree (LSM)
    - LSM trees are core data structure behind databases such as BigTable, Cassandra and RocksDB
------------------------------------------------------------------------------------------------------------------------
+++Chapter 25: S3-like Object Storage

hierarchical directory structure

Object storage: targets relatively cold data / mainly used for archival and backup / in a flat structure / data access is normally provided via Restful API
AWS S3 / Google object storage / Azure blob storage

IAM: Identity and access management

This is a critical service, so we suggest building a cluster of 5 or 7 replacement service nodes with Paxos or Raft consensus protocol
the consensus protocol ensures that as long as more than half of the nodes are healthy, the service as a whole continues to work

Consistent hashing is a common implementation of such as lookup function

SQLite is a C-language library that implements a small, fast, self-contained, high-reliability, full-featured, file nased SQL database engine

use the hash of (bucket_name, object_name) as sharding key
------------------------------------------------------------------------------------------------------------------------
+++Chapter 26: Real-time Gaming Leaderboard

call the server explicitly
without any client intervention

ROWNUM: ROWNUM numbers the records in a result set (sql)
The ROW_NUMBER() is a window function that assigns a sequential integer to each row within the partition of a result set.

select t.[Email], t.[LocationId], t.rownum from ( SELECT  ROW_NUMBER() OVER (order by [LocationId] ) as rownum , [RequestId]
      ,[Email]
      ,[LocationId]
  FROM [EEV1_ProSYNC].[dbo].[Extract_Request]) t where t.rownum < 10

select t.[Email], t.[LocationId], t.rownum from ( SELECT  ROW_NUMBER() OVER (partition by [LocationId] order by [Email] ) as rownum , [RequestId]
    ,[Email]
    ,[LocationId]
FROM [EEV1_ProSYNC].[dbo].[Extract_Request]) t where t.rownum < 10

A relational database is not designed to handle the high load of read queries this implementation would require
One optimization is to add index and limit the number of pages to scan with LIMIT clause

Redis solution
Redis has a specific data type called sorted sets that are ideal for solving leaderboard system design problems
Each member of sorted set is associated with a score, and this score is used to rank the sorted set in ascending order

Sorted sets are more performant since each element is automatically positioned in the right order during inert or updates.
The complexity of add or find operation in sorted set is logarithmic: O(log(n))

ZADD: insert into set if it doesnt exist / O(log(n))
ZINCRBY: increment by the specific increment / O(log(n))
ZRANGE/ZREVRANGE: fetch a range sorted by score /  O(log(n) + m)
ZRANK/ZRERANK: fetch the position of any user sorting in ascending/descending order in logarithmic time

This is well within the performance envelope of a single Redis server
Redis supports persistence

If this becomes too inefficient in the long term, we can leverage a user profile cache to store users' details fo the top 10 players
spin up a server instance

Data sharding:
   - fixed partitions: break up the data by range (preferred)
   - hash partitions: hash slot / CRC16(key)%16384
                      an update would simply change the score of the user in the corresponding shard
                      Retrieving the top 10 players on the leaderboard is more complicated
                      Scatter/Gather: Gather the top 10 players from each shard and have the application sort the data
                      Those queries can be parallelized to reduce latency

NoSQL has the following properties:
   - Optimized for writes
   - Efficiently sort items within the same partition by score

AWS DynamoDB: Global Secondary Index
                 - Partition Key (PK): game_name#{year-month}
                 - Sort Key: score

There is a trade-off between load on partitions and read complexity
relative ranking (say 90th percentile)
------------------------------------------------------------------------------------------------------------------------
+++Chapter 27: Payment System

third-party payment processors, such as PayPal, Stripe, Braintree, Square, etc. / Payment Service Provide (PSP)
high security and compliance requirement
risk check / assess for compliance with regulations
perform reconciliation and fix any inconsistencies

Pay-in flow / Pay-out flow
Buyer Credit Card -> Pay-in -> E-commerce Website Bank Account -> Pay-out -> Seller Bank Account

Card schemes: Visa, MasterCard
Ledger: keep a financial record of the payment transaction

payment_order_id is de-duplication ID, also called idempotency key
checkout_id is the foreign key

double-entry ledger system/accounting/bookkeeping / Debit / Credit
use hosted credit pages provided by PSPs

payment system uses third-party account payable providers like Tipalti to handle pay-outs
reconciliation: settlement file

exactly-once delivery
    - implement at-least-once using retry
        > immediate retry
        > fixed intervals
        > incremental intervals
        > exponential backoff
        > cancel
    - implement at-most-once-using idempotency check
        > idempotency: clients can make the same call repeatedly and produce the same result
                       an idempotency key is usually a unique value that is generated by client and expires after a certain period of time
                       UUID is commonly used as an idempotency key
                       in e-commerce website, idempotency key is usually ID of shopping cart right before checkout
                       the same idempotency key is detected, only one request is processed and received '429 Too Many Requests' status code
                       use unique key constraint to support idempotency

replication lag cause inconsistent data between primary database and replicas. two options to solve:
    - Serve both reads and writes from primary database only. replicas to ensure data reliability, not serve any traffic
    - Ensure all replicas are always in-sync. Use consensus algorithms such as Paxos and Raft. Or use consensus-based distributed databases such as YugabyteDB or CockroachDB
------------------------------------------------------------------------------------------------------------------------
+++Chapter 28: Digital Wallet

cross-wallet balance transfer operation
availability requirement is 99.99%
deduct $1 from the account

a relational database running on a typical data center node can support a few thousand transactions per second
Three high level designs:
   - Simple in-memory solution
   - Database-based distributed transaction solution
   - Event sourcing solution with reproducibility

In-memory sharding solution
   - Redis. But one Redis node is not enough to handle 1 million TPS. We need to set up a cluster of Redis nodes and evenly distribute user accounts among them (partitioning or sharding)
   - Zookeeper as highly-available configuration storage solution. The number of partitions and addresses of all Redis nodes are stored in a centralized place. Zookeeper is used to maintain the sharding information
   - Stateless wallet service uses sharding information to locate Redis nodes and updates account balances accordingly

Distributed transactions
    - Database sharding
        > replace Redis node with a transactional relational database node. Make updates to two different storage nodes atomic
    - Database transaction: two-phase commit
        > a transaction may involve multiple processes on multiple nodes
        > two-phase commit (2PC)
        > the coordinator is the wallet service
        > X/Open XA standard that coordinates heterogeneous databases to achieve 2PC
        > 2PC is not performant and can be a single point of failure
    - Distributed transaction: Try-Confirm/Cancel (TC/C)
        > Try-Confirm & Try-Cancel (undo: using a reverse operation to offset the previous transaction result when an error occurs)
        > two phases in 2PC are wrapped in the same transaction, but in TC/C each phase is a separate transaction
    - Distributed transaction: Saga
       > Saga is the de-facto standard in microservice architecture
       > linear order execution
       > compensating transactions: roll back from the current operation to the first operation in reverse order
       > two ways to coordinate the operations:
           Choreography: subscribing to other service's events. fully decentralized coordination (fully asynchronous / has to maintain internal state machine / hard to manage)
           Orchestration: a single coordination instructs all services (preferred)

Event sourcing
   - audit capability / conduct data audit easily
   - command / event / state/ state machine
   - Kafka is used as command queue
   - Reproducibility: the most important advantage that event sourcing has over other architectures
                      reconstruct historical balance states by replaying the events from the very beginning
   - Command-query responsibility segregation (CQRS)
       > all valid business records are saved in an immutable Event queue which could be used for correctness verification
       > audit trail to reconcile the financial records

High-performance event sourcing
   - File-based command and event list
       > event list uses an append-only data structure
       > Appending is a sequential write operation that is generally very fast
       > cache recent commands and events in memory
   - File-based state
       > SQLite that is a file-based local relational database
       > RocksDB that is a local file-based ley-value store
       > RocksDB is chosen because it uses a log-structure merge-tree (LSM) that is optimized for write operations
   - snapshot
       > snapshot is an immutable view of historical state
       > snapshot is a giant binary file and a common solution is to save it in an object storage solution, such as HDFS
       > Once a snapshot is saved, the state machine doesn't have to restart from the very beginning

Reliable high-performance event sourcing
   - consensus
       > no data lost
       > the relative order of data within a log file remains the same across nodes
       > consensus-based replication is a good fit
       > use Raft consensus algorithm as an example (leader / follower / candidate) / sync data between nodes
       > to increase system reliability, we use Raft consensus algorithm to replicate even lust onto multiple nodes

Distributed event sourcing
   Pull vs Push
       - Pull: not real-time and may overload wallet service if polling frequency is set too high
               the naive pull model can be improved by adding a reverse proxy between external user and event sourcing node (still not real-time, but simplifies the client logic)
       - Push: read-only state machine pushes execution status back to reverse proxy (real-time response)
   Distributed transaction
       - TC/C or Saga protocol ise used to coordinate Command executions across multiple node groups
------------------------------------------------------------------------------------------------------------------------
+++Chapter 28: Stock Exchange

the order need to be withheld to prevent overspending
legal compliance
prevent distributed denial-of-service (DDos)

Broker: Charles Schwab, Robinhood, Etrade, Fidelity
Limit order / Market order / Institutional client / Market data levels / Candlestick chart
the best bid price / ask price / the filled order

write the consolidated records to the database

Trading flow:
   - matching engineer (cross engineer): order book / symbol / in a deterministic order / the same sequence of executions (fills)
   - sequencer: key component to make matching engineer deterministic / sequencer ID
                The incoming orders and outgoing executions are stamped with sequence IDs for 3 reasons:
                    > Timeliness and fairness
                    > fast recovery / replay
                    > exactly-once guarantee
   - order manager: Event sourcing is the perfect design for an order manager
   - client gateway: Auth / Validation / Rate Limit/ Normalization / FIXT Support
                     the main considerations are latency / transaction volume / security requirements
Market data flow
Reporting flow
   - trading history/ tax reporting / compliance reporting / settlement

constant lookup time / query / iterate through
achieve O(1) time complexity by using doubly-linked list
a good measure for the level of stability is the 99th percentile latency
the round trip network latency
add up to single-digit milliseconds / tens of milliseconds
reduce | eliminate network and disk access latency / eliminate the network hops by putting everything on the same server
low 99th percentile latency

CPU pinning / mmap: a POXIS-compliant UNIT system called name mmap(2) that maps a file into the memory of a process

in event sourcing, instating of storing the current states, it keeps an immutable log og all state-changing events
These events are the golden source of truth

redundant instance / backup instance
hot-warm design / we could use reliable UDP to efficiently broadcast the event messages to all warm servers
fault tolerance: tackle it by replicating core data to data centers in multiple cities
it mitigates the risk of a natural disaster such as an earthquake or a large-scale power outage
decide to failover to the backup instance / choose leader among backup instances / recovery time / operate under degraded conditions

Recovery Time Objective (RTO) is the maximum acceptable amount of time for restoring a network or application and regaining access to data after an unplanned disruption.
Recovery point objective (RPO) is the maximum acceptable amount of data loss after an unplanned data-loss incident, expressed as an amount of time/ loss tolerance

split vote: multiple followers become candidates at the same time
categorize services based on priority and define a degradation strategy to maintain a minimum service level

