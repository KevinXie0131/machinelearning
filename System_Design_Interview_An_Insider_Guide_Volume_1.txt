Fallback (provides an alternative solution during a service request failure)
circuit breaker
Sharding key / Partition kay
    - Re-sharding data
    - Celebrity problem
    - Join and de-normalization

support millions of users:
    • Keep web tier stateless
    • Build redundancy at every tier
    • Cache data as much as you can
    • Support multiple data centers
    • Host static assets in CDN
    • Scale your data tier by sharding
    • Split tiers into individual services
    • Monitor your system and use automation tools

sorted by reverse chronological order
fanout service (Fan-out is the distribution of messages by a service or message router to multiple users, often simultaneously)
fanout worker

authentication / rate limiting
roll out the system / a recap of your design
------------------------------------------------------------------------------------------------------------------------

throttle rules / rate limiter middleware / malicious actors
API gateway / IP whitelisting
Algorithms for rate limiting
    • Token bucket
        pre-defined capacity
    • Leaking bucket
    • Fixed window counter
    • Sliding window log
    • Sliding window counter

at one minute interval / allows a burst of traffic for short periods
increments the counter by one / reaches the pre-defined threshold
the available quota / remove all the outdated timestamps
rounded up or down

In-memory cache / supports time-based expiration strategy
fetches the counter

two challenges in a distributed environment:
    • Race condition
    • Synchronization issue

gracefully recover from exceptions
------------------------------------------------------------------------------------------------------------------------

Consistent hashing (hash ring / virtual nodes or replicas / redistributed.)
mitigate this problem
Distribute data across multiple servers evenly / Minimize data movement
------------------------------------------------------------------------------------------------------------------------

Automatic scaling
due to the space constraint
stale data
Data partition / Data replication / Write path / Read path
Quorum consensus

If W + R > N, strong consistency is guaranteed because there must be at least one overlapping node that has the latest data to ensure consistency.
    N = The number of replicas
    W = A write quorum of size W
    R = A read quorum of size R

Consistency models:
    • Strong consistency (usually achieved by forcing a replica not to accept new reads/writes until every replica has agreed on current write)
    • Weak consistency
    • Eventual consistency (this is a specific form of weak consistency. Given enough time, all updates are propagated, and all replicas are consistent.)

Dynamo and Cassandra adopt eventual consistency, which is our recommended consistency model for our key-value store.

Inconsistency resolution: versioning / vector locks
detect conflicts and reconcile conflicts
A vector clock is a [server, version] pair associated with a data item

Failure detection
    gossip protocol (decentralized failure detection methods) - node membership list / heartbeat counter

Handling temporary failures
    'sloppy quorum' is used to improve availability (chooses the first W healthy servers for writes and first R healthy servers for reads on the hash ring. Offline servers are ignored)




