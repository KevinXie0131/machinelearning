YouTube - Building custom data stores like Vitess and BigTable to meet scalability needs

video streaming service / content creator / content viewer / monthly active users / storage space
deliver content effectively to the client and avoid network congestions

High availability: Generally, an uptime of 99% and above is considered good
Scalability: storage / bandwidth / number of concurrent user requests
Good performance:
Reliability: should not be lost or damaged.
don't require strong consistency for YouTube's design

We calculate bandwidth in bits per second (bps)
Encoders and transcoders compress videos and transform them into different formats and qualities to support varying numbers of devices according to their screen resolution and bandwidth.
stores the metadata / hands over the video to the encoder for encoding
blob storage (similar to GFS or S3)
with the least possible lag

Web servers: Web servers take in user requests and respond to them / interface to our API servers / Apache or Lighttpd
             Lighttpd is preferable because it can serve static pages and videos due to its fast speed.
             decouple clients' services from the application and business logic.
Application server: The application and business logic resides in application servers.
Bigtable: Bigtable is a good choice for storing thumbnails because of its high throughput and scalability for storing key-value data.
          Bigtable is optimal for storing a large number of data items each below 10 MB.
colocation sites: Colocation centers are used where it’s not possible to invest in a data center facility due to business reasons.

for optimal access time / sharding / we scale and do frequent writes on the database
keywords will be extracted from the documents and stored in a key-value store.

Low latency/Smooth streaming: Geographically distributed cache servers at the ISP level
                              distributed cache management syste
                              content delivery networks (CDNs)
Scalability: horizontal scalability of web and application servers
Availablity: by replicating data to as many servers as possible to avoid a single point of failure
             Replicating data across data centers will ensure high availability
             local load balancers can exclude any dead servers,
             and global load balancers can steer traffic to a different region if the need arises.
Reliability: by using data partitioning and fault-tolerance techniques.
             heartbeat / consistent hashing to add or remove servers seamlessly and reduce the burden on specific servers in case of non-uniform load.

Trade-offs:
    Consistency: This is because we don't need to show a consistent feed to all the users.
    Distributed cache: Memcached is a good choice because it is open source and uses the popular Least Recently Used (LRU) algorithm.
                       Since YouTube video access patterns are long-tailed, LRU-like algorithms are suitable for such data sets.
    Bigtable versus MySQL: One could use alternatives to GFS and Bigtable, such as HDFS and Cassandra.
    Public versus private CDN: This choice is more of a cost issue than a design issue.
    Duplicate videos: copyright issue / simple techniques like locality-sensitive hashing
                      complex techniques like Block Matching Algorithms (BMAs) and phase correlation / artificial intelligence (AI)

become a choke point / out-of-the-box solutions
However, data denormalization won’t work because it comes at the cost of reduced writing performance.
   Even if our work is read-intensive, as the system scales, writing performance will degrade to an unbearable limit.

divide videos into shorter time frames and refer to them as segments -> generate different files called chunks
small -> medium -> large chunk size: increasing bitrate
result in the congestion of networks

Adaptive streaming: when the bandwidth is high, a higher quality chunk is sent to the client and vice versa
adaptive bitrate algorithm depends on the following four parameters:
   End-to-end available bandwidth (from a CDN/servers to a specific client).
   The device capabilities of the user.
   Encoding techniques used.
   The buffer space at the client.
------------------------------------------------------------------------------------------------------------------------
Quora - Vertical sharding of MySQL database to meet the scalability requirements

Quora is a social question-and-answer service / more conversational and can result in deeper understanding
The manager processes distribute work among the worker processes using a router library
The router library is enqueued with tasks by the manager processes and dequeued by worker processes.
Each application server maintains several in-memory queues to handle different user requests.

a relational database like MySQL because it offers a higher degree of consistency.
HBase can be a good option to store and retrieve data at high bandwidth
   We require high read/write throughput because big data processing systems use high parallelism to efficiently get the required statistics.
   suitable for storing a large amount of small-sized data.
Also, blob storage is required to store videos and images posted in questions and answers.

Distributed cache: Memcached and Redis
CDNs serve frequently accessed videos and images.

Task prioritization is performed by employing different queues for different tasks.
offline mode poses a lesser burden on the infrastructure

Vertical sharding of MySQL (horizontal sharding is more common)
A partition has a single primary server and multiple replica servers.
maintained by a service like ZooKeeper
the number of read-replicas can be increased for hot shards
For edge cases where joining may be needed, we can perform it at the application level.

Kafka: reduces the request load on service hosts by separating not-so-urgent tasks from the regular API calls.
       Each of these jobs is executed through cron jobs.
       use sharded counters as an effective solution to the view counter problem

Features like comments, upvotes, and downvotes require frequent page updates from the client side.
    - Polling: the server may get uselessly overburdened.
    - long polling: the server may not respond for as long as 60 seconds if there are no updates
                    the server makes client wait until fresh content is available
we prefer sockets due to their high decoupling, flow control, and ability to work for both single servers or over the network.
WebSockets are another low-latency solution with low overhead.
    However, WebSockets might be an overkill for the features offered by Quora.

Scalability: The horizontal scaling of these service hosts is convenient because they are homogeneous.
             To reduce complex join queries, tables anticipating join operations are placed in the same shard or partition.
Consistency: certain critical data like questions and answers should be stored synchronously
             For such cases, eventual consistency is favored for improved performance.
             Offers eventual consistency for non-critical data like the view counter.
Availability: isolation between different components, keeping redundant instances, using CDN,
              using configuration services like ZooKeeper, and load balancers to hide failures from users.
              disaster recovery management / Load balancers hide server failures from end users.
Performance: Sharding improves QPS of MySQL.
             Custom, in-memory caching system reduces the latency of frequently accessed data.
Disaster recovery: frequent backups / Quick restoration of backed-up data in a timely manner completes a disaster recovery plan.
------------------------------------------------------------------------------------------------------------------------
Google Maps - The use of segmentation of a map to meet scalability needs and achieve high performance

Functional requirements:
    - Identify the current location
    - Recommend the fastest route
    - Give directions
Non-functional requirements:
    - Availability: highly available
    - Scalability: Partition the large graphs into small graphs to ease segment addition.
                   Host the segments on different servers to enable serving more queries per second.
    - Less response time: calculate the ETA and the route, given the source and the destination points.
                          Cache the processed graphs.
                          Use a key-value store to quickly get the required information.
    - Accuracy: Collect live data.

Road networks are modeled with a graph data structure, where intersection points are the vertices,
and the roads between intersections are the weighted edges.

Number of servers estimation: Number of requests a single server can handle per second: 8,000
Storage estimation: over 20 petabytes as of 2022
Bandwidth estimation: incoming and outgoing traffic

We'll use Kafka as a pub-sub system in our design.

The route finder forwards the requests to an area search service with the source and the destination points
The area search service uses the distributed search to find the latitude/longitude for the source and the destination.
After finding the area, the area search service asks the graph processing service to process part of the graph, depending on the area to find the optimal path
The graph processing service fetches the edges and nodes within that specified area from the database, finds the shortest path
The direction request is handled by the navigator

Scalability is about the ability to efficiently process a huge road network graph
    - The idea is to break down a large graph into smaller sub-graphs, or partitions.
    - So, we divide the globe into small parts called segments. Each segment corresponds to a subgraph

The most common shortest path algorithm is the Dijkstra’s algorithm.
With the source and destination points, we can find the aerial distance between them using the haversine formula
Aerial distance is the distance between two places measured in a straight line through the air rather than using roads on the ground.

Graph database: The road network inside the segment in the form of a graph.
Relational DB: We store the information to determine whether, at a particular hour of the day, the roads are congested.

improve the ETA estimation accuracy using live data
Google Maps uses a combination of GPS, Wi-Fi, and cell towers to track users' locations.

WebSocket is a communication protocol that allows users and servers to have a two-way, interactive communication session.
   This helps in the real-time transfer of data between user and server.
Google Maps uses lazy loading of data, putting less burden on the system, which improves availability.
memory issues loading and processing a huge graph:
   - We divided the world into small segments. Each segment is hosted on a different server in the distributed system.
     The user requests for different routes are served from the different segment servers.
improve scalability by non-uniformly selecting the size of a segment:
   - selecting smaller segment sizes in densely connected areas
   - selecting bigger segments for the outskirts.
------------------------------------------------------------------------------------------------------------------------
Yelp - Usage of Quadtrees for speedy access to spatial data

Services based on proximity servers are helpful in finding nearby attractions such as restaurants
in a given radius with minimum latency.
There can be two types of users: business owners / other users
based on their GPS location (longitude, latitude)

This process returns a JSON object that contains a list of all the possible items in the specified category that also fall within the specified radius.
    Each entry has a place name, address, category, rating, and thumbnail.
This process returns a JSON object that contains information of the specified place.

We generate IDs using the unique ID generator.

Segments producer: This component is responsible for communicating with the third-party world map data services (for example, Google Maps)
QuadTree servers: finds a list of places based on the given radius
Aggregators: The QuadTrees accumulate all the places and send them to the aggregators.
SQL database: The data in these tables is inherently relational and structured.
              SQL-based databases are better suited to have a consistent view of the data
Key-value stores: We'll need to fetch the places in a segment efficiently.
                  For that, we store the list of places against a segment ID in a key-value store to minimize searching time.
                  We also save the QuadTree information in the key-value store, by storing the QuadTree data against a unique ID.
Load balancer: A load balancer distributes users' incoming requests to all the servers uniformly.

If we consider our radius to be ten miles, then we’ll have 6 million segments. An 8-Byte identifier will identify each segment.
Dynamic segments: We solve the problem of uneven distribution of places in a segment by dynamically sizing the segments.
Search using a QuadTree: All the leaf nodes in the QuadTree are linked, just like in a doubly-linked list
                         The leaf node will explore its neighboring nodes to find more cafes
                         We can easily store a QuadTree on a server.
------------------------------------------------------------------------------------------------------------------------
Uber - Improved payment service to ensure fraud detection, and matching the driver and rider on maps
------------------------------------------------------------------------------------------------------------------------
Twitter - The use of client-side load balancers for multiple services that had thousands of instances in order to reduce latency
------------------------------------------------------------------------------------------------------------------------
Newsfeed - A recommendation system to ensure ranking and feed suggestions
------------------------------------------------------------------------------------------------------------------------
Instagram - A perfect example of how different building blocks combine to build a scalable and performant system
------------------------------------------------------------------------------------------------------------------------
TinyURL - Encoding IDs in the base-58 system for generating unique short URLs
------------------------------------------------------------------------------------------------------------------------
Web crawler - Detection, identification, and resolution of Web crawler traps
------------------------------------------------------------------------------------------------------------------------
WhatsApp - Message management for offline users
------------------------------------------------------------------------------------------------------------------------
Typeahead - The usage of an efficient trie data structure to provide suggestions
------------------------------------------------------------------------------------------------------------------------
Google Docs - Concurrency management for simultaneous writes, using techniques like operational transformation (OT) and Conflict-free Replicated Data Type (CRDT)
------------------------------------------------------------------------------------------------------------------------
