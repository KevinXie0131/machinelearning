YouTube - Building custom data stores like Vitess and BigTable to meet scalability needs

video streaming service / content creator / content viewer / monthly active users / storage space
deliver content effectively to the client and avoid network congestions

High availability: Generally, an uptime of 99% and above is considered good
Scalability: storage / bandwidth / number of concurrent user requests
Good performance:
Reliability: should not be lost or damaged.
don't require strong consistency for YouTube's design

We calculate bandwidth in bits per second (bps)
Encoders and transcoders compress videos and transform them into different formats and qualities to support varying numbers of devices according to their screen resolution and bandwidth.
stores the metadata / hands over the video to the encoder for encoding
blob storage (similar to GFS or S3)
with the least possible lag

Web servers: Web servers take in user requests and respond to them / interface to our API servers / Apache or Lighttpd
             Lighttpd is preferable because it can serve static pages and videos due to its fast speed.
             decouple clients' services from the application and business logic.
Application server: The application and business logic resides in application servers.
Bigtable: Bigtable is a good choice for storing thumbnails because of its high throughput and scalability for storing key-value data.
          Bigtable is optimal for storing a large number of data items each below 10 MB.
colocation sites: Colocation centers are used where it’s not possible to invest in a data center facility due to business reasons.

for optimal access time / sharding / we scale and do frequent writes on the database
keywords will be extracted from the documents and stored in a key-value store.

Low latency/Smooth streaming: Geographically distributed cache servers at the ISP level
                              distributed cache management syste
                              content delivery networks (CDNs)
Scalability: horizontal scalability of web and application servers
Availablity: by replicating data to as many servers as possible to avoid a single point of failure
             Replicating data across data centers will ensure high availability
             local load balancers can exclude any dead servers,
             and global load balancers can steer traffic to a different region if the need arises.
Reliability: by using data partitioning and fault-tolerance techniques.
             heartbeat / consistent hashing to add or remove servers seamlessly and reduce the burden on specific servers in case of non-uniform load.

Trade-offs:
    Consistency: This is because we don't need to show a consistent feed to all the users.
    Distributed cache: Memcached is a good choice because it is open source and uses the popular Least Recently Used (LRU) algorithm.
                       Since YouTube video access patterns are long-tailed, LRU-like algorithms are suitable for such data sets.
    Bigtable versus MySQL: One could use alternatives to GFS and Bigtable, such as HDFS and Cassandra.
    Public versus private CDN: This choice is more of a cost issue than a design issue.
    Duplicate videos: copyright issue / simple techniques like locality-sensitive hashing
                      complex techniques like Block Matching Algorithms (BMAs) and phase correlation / artificial intelligence (AI)

become a choke point / out-of-the-box solutions
However, data denormalization won’t work because it comes at the cost of reduced writing performance.
   Even if our work is read-intensive, as the system scales, writing performance will degrade to an unbearable limit.

divide videos into shorter time frames and refer to them as segments -> generate different files called chunks
small -> medium -> large chunk size: increasing bitrate
result in the congestion of networks

Adaptive streaming: when the bandwidth is high, a higher quality chunk is sent to the client and vice versa
adaptive bitrate algorithm depends on the following four parameters:
   End-to-end available bandwidth (from a CDN/servers to a specific client).
   The device capabilities of the user.
   Encoding techniques used.
   The buffer space at the client.
------------------------------------------------------------------------------------------------------------------------
Quora - Vertical sharding of MySQL database to meet the scalability requirements

Quora is a social question-and-answer service / more conversational and can result in deeper understanding
The manager processes distribute work among the worker processes using a router library
The router library is enqueued with tasks by the manager processes and dequeued by worker processes.
Each application server maintains several in-memory queues to handle different user requests.

a relational database like MySQL because it offers a higher degree of consistency.
HBase can be a good option to store and retrieve data at high bandwidth
   We require high read/write throughput because big data processing systems use high parallelism to efficiently get the required statistics.
   suitable for storing a large amount of small-sized data.
Also, blob storage is required to store videos and images posted in questions and answers.

Distributed cache: Memcached and Redis
CDNs serve frequently accessed videos and images.

Task prioritization is performed by employing different queues for different tasks.
offline mode poses a lesser burden on the infrastructure

Vertical sharding of MySQL (horizontal sharding is more common)
A partition has a single primary server and multiple replica servers.
maintained by a service like ZooKeeper
the number of read-replicas can be increased for hot shards
For edge cases where joining may be needed, we can perform it at the application level.

Kafka: reduces the request load on service hosts by separating not-so-urgent tasks from the regular API calls.
       Each of these jobs is executed through cron jobs.
       use sharded counters as an effective solution to the view counter problem

Features like comments, upvotes, and downvotes require frequent page updates from the client side.
    - Polling: the server may get uselessly overburdened.
    - long polling: the server may not respond for as long as 60 seconds if there are no updates
                    the server makes client wait until fresh content is available
we prefer sockets due to their high decoupling, flow control, and ability to work for both single servers or over the network.
WebSockets are another low-latency solution with low overhead.
    However, WebSockets might be an overkill for the features offered by Quora.

Scalability: The horizontal scaling of these service hosts is convenient because they are homogeneous.
             To reduce complex join queries, tables anticipating join operations are placed in the same shard or partition.
Consistency: certain critical data like questions and answers should be stored synchronously
             For such cases, eventual consistency is favored for improved performance.
             Offers eventual consistency for non-critical data like the view counter.
Availability: isolation between different components, keeping redundant instances, using CDN,
              using configuration services like ZooKeeper, and load balancers to hide failures from users.
              disaster recovery management / Load balancers hide server failures from end users.
Performance: Sharding improves QPS of MySQL.
             Custom, in-memory caching system reduces the latency of frequently accessed data.
Disaster recovery: frequent backups / Quick restoration of backed-up data in a timely manner completes a disaster recovery plan.
------------------------------------------------------------------------------------------------------------------------
Google Maps - The use of segmentation of a map to meet scalability needs and achieve high performance

Functional requirements:
    - Identify the current location
    - Recommend the fastest route
    - Give directions
Non-functional requirements:
    - Availability: highly available
    - Scalability: Partition the large graphs into small graphs to ease segment addition.
                   Host the segments on different servers to enable serving more queries per second.
    - Less response time: calculate the ETA and the route, given the source and the destination points.
                          Cache the processed graphs.
                          Use a key-value store to quickly get the required information.
    - Accuracy: Collect live data.

Road networks are modeled with a graph data structure, where intersection points are the vertices,
and the roads between intersections are the weighted edges.

Number of servers estimation: Number of requests a single server can handle per second: 8,000
Storage estimation: over 20 petabytes as of 2022
Bandwidth estimation: incoming and outgoing traffic

We'll use Kafka as a pub-sub system in our design.

The route finder forwards the requests to an area search service with the source and the destination points
The area search service uses the distributed search to find the latitude/longitude for the source and the destination.
After finding the area, the area search service asks the graph processing service to process part of the graph, depending on the area to find the optimal path
The graph processing service fetches the edges and nodes within that specified area from the database, finds the shortest path
The direction request is handled by the navigator

Scalability is about the ability to efficiently process a huge road network graph
    - The idea is to break down a large graph into smaller sub-graphs, or partitions.
    - So, we divide the globe into small parts called segments. Each segment corresponds to a subgraph

The most common shortest path algorithm is the Dijkstra’s algorithm.
With the source and destination points, we can find the aerial distance between them using the haversine formula
Aerial distance is the distance between two places measured in a straight line through the air rather than using roads on the ground.

Graph database: The road network inside the segment in the form of a graph.
Relational DB: We store the information to determine whether, at a particular hour of the day, the roads are congested.

improve the ETA estimation accuracy using live data
Google Maps uses a combination of GPS, Wi-Fi, and cell towers to track users' locations.

WebSocket is a communication protocol that allows users and servers to have a two-way, interactive communication session.
   This helps in the real-time transfer of data between user and server.
Google Maps uses lazy loading of data, putting less burden on the system, which improves availability.
memory issues loading and processing a huge graph:
   - We divided the world into small segments. Each segment is hosted on a different server in the distributed system.
     The user requests for different routes are served from the different segment servers.
improve scalability by non-uniformly selecting the size of a segment:
   - selecting smaller segment sizes in densely connected areas
   - selecting bigger segments for the outskirts.
------------------------------------------------------------------------------------------------------------------------
Yelp - Usage of Quadtrees for speedy access to spatial data

Services based on proximity servers are helpful in finding nearby attractions such as restaurants
in a given radius with minimum latency.
There can be two types of users: business owners / other users
based on their GPS location (longitude, latitude)

This process returns a JSON object that contains a list of all the possible items in the specified category that also fall within the specified radius.
    Each entry has a place name, address, category, rating, and thumbnail.
This process returns a JSON object that contains information of the specified place.

We generate IDs using the unique ID generator.

Segments producer: This component is responsible for communicating with the third-party world map data services (for example, Google Maps)
QuadTree servers: finds a list of places based on the given radius
Aggregators: The QuadTrees accumulate all the places and send them to the aggregators.
SQL database: The data in these tables is inherently relational and structured.
              SQL-based databases are better suited to have a consistent view of the data
Key-value stores: We'll need to fetch the places in a segment efficiently.
                  For that, we store the list of places against a segment ID in a key-value store to minimize searching time.
                  We also save the QuadTree information in the key-value store, by storing the QuadTree data against a unique ID.
Load balancer: A load balancer distributes users' incoming requests to all the servers uniformly.

If we consider our radius to be ten miles, then we’ll have 6 million segments. An 8-Byte identifier will identify each segment.
Dynamic segments: We solve the problem of uneven distribution of places in a segment by dynamically sizing the segments.
Search using a QuadTree: All the leaf nodes in the QuadTree are linked, just like in a doubly-linked list
                         The leaf node will explore its neighboring nodes to find more cafes
                         We can easily store a QuadTree on a server.
------------------------------------------------------------------------------------------------------------------------
Uber - Improved payment service to ensure fraud detection, and matching the driver and rider on maps

Uber is an application that provides ride-hailing services to its users.
High-level Design / Detailed Design / Payment Service and Fraud Detection (detect any fraudulent activity related to payment)
Confirm pickup / Show trip updates / estimated time of arrival (ETA)
retract the ride offer
handle an ever-increasing number of drivers / provide fast and error-free services
strongly consistent: The drivers and riders in an area should have a consistent view of the system.

API design
   - Update driver location
   - Find nearby drivers
   - Request a ride
   - Show driver ETA
   - Confirm pickup
   - Show trip updates
   - End the trip

The rider enters the drop-off location and requests a ride.
Until a matching driver is found, the status will be "Waiting for the driver to respond."
The drivers report their location every four seconds.

typeOfVehicle: for example, business, economy, and so on.

Location manager: shows the nearby drivers to the riders
                  This service also receives location updates from the drivers every four seconds.
                  The location of drivers is then communicated to the QuadTree map service to determine which segment the driver belongs to on a map.
                  The location manager saves the last location of all drivers in a database and saves the route followed by the drivers on a trip.
QuadTree map service: updates the location of the drivers.
                      The main problem is how we deal with finding nearby drivers efficiently:
                          our dynamic segment solution:
                             To identify the driver's new location, we must first find a proper grid depending on the driver's previous position.
                             If the new location doesn't match the current grid, we should remove the driver from the current grid and shift it to the correct grid.
                             We have to repartition the new grid if it exceeds the driver limit, which is the number of drivers for each region that we set initially.
                             we can use a hash table to store the latest position of the drivers and update our QuadTree occasionally, say after 10–15 seconds.
                             We can update the driver's location in the QuadTree around every 15 seconds instead of four seconds,

A QuadTree is a tree data structure in which each internal node has exactly four children.
   QuadTrees are the two-dimensional analog of octrees and are most often used to partition a two-dimensional space by recursively subdividing it into four quadrants or regions.
   The data associated with a leaf cell varies by application, but the leaf cell represents a "unit of interesting spatial information".

ETA service: considers factors such as route and traffic.
The whole road network is represented as a graph.
   - Intersections are represented by nodes, while edges represent road segments.
   - The graph also depicts one-way streets, turn limitations, and speed limits.

utilize routing algorithms such as Dijkstra's algorithm (operates on top of an unprocessed graph, is quite slow for such a system)
preprocess the optimum path inside partitions using contraction hierarchies (all the partitions process the best route in parallel)

Contraction hierarchies is a speed-up method optimized to exploit the properties of graphs representing road networks.
   The speed-up is achieved by creating shortcuts in a preprocessing phase, which are then used during a shortest-path query to skip over unimportant vertices

DeepETA: machine learning component named DeepETA to deliver an immediate improvement to metrics in production

use Cassandra to store the driver’s last known location and the trip information
   As the data becomes massive with the passage of time, it reduces our system's performance. Also, it won't be easy to scale the MySQL database.
use a MySQL database to store trip information while it's in progress.
   We use MySQL for in-progress trips for frequent updates since the trip information is relational, and it needs to be consistent across tables.
   The data we store for each trip is highly relational, it's spread over multiple tables, and it must be identical among all tables.
   MySQL helps to store this relational data and keep it consistent across tables.

Google Cloud Spanner provides global transactions, strongly consistent reads, and automatic multisite replication and failover features.

Fault tolerance: primary-secondary replication model (synchronously replicate data from primary to secondary databases)
Load balancers: uniformly distribute the load among the servers.
Cache: first store the updated location in the hash table stored in Redis.
       Eventually, these values are copied into the persistent storage every 10–15 seconds.

secure / guard the system against malicious activities
fraudulent activities in the context of payment

Major functionality of payment:
   - New payment options: It adds new payment options for users.
   - Authorization: It authorizes the payments for transactions.
   - Refund: It refunds a payment that was authorized before.
   - Charging: It moves money from a user account to Uber.
What to prevent:
   - Lack of payment
   - Duplicate payments
   - Incorrect payment
   - Incorrect currency conversion
   - Dangling authorization
The Uber payment platform is based on the double-entry bookkeeping method.

API / Order store / Account store / Risk engine / Payment authorization service / User profile service
Payment profile service: This provides information on the payment mechanisms, such as credit and debit cards (for record-keeping)
PSP gateways: This connects with payment service providers

authorization token
Apache Kafka is an open-source stream-processing software platform. It’s the primary technology used in payment services.
The key capabilities of Kafka that the payment service uses are the following:
   - It works like message queues to publish and subscribe to message queues.
   - It stores the records in a way that is fault tolerant.
   - It processes the payment records asynchronously.

Fraud detection is a critical operational component of our design.
Fraud detection is challenging because many instances of fraud are like detecting zero-day security bugs.

highly available: We used WebSocket servers
Our services are decoupled and isolated, which eventually increases the reliability.
------------------------------------------------------------------------------------------------------------------------
Twitter - The use of client-side load balancers for multiple services that had thousands of instances in order to reduce latency

Twitter is a free microblogging social network where registered users post messages called 'Tweets'.
Search Tweets: search Tweets by typing keywords, hashtags, or usernames in the search bar
View user or home timeline: Registered users can view the user’s timeline, which contains their own Tweets.
Retweet a Tweet: Registered users can Retweet public Tweets of other users on Twitter.

highly available: communicate time-sensitive information (service outage messages) / have a good uptime percentage.
Latency: near real-time services, such as Tweet distribution to followers, must have low latency.
Scalability: The workload on Twitter is read-heavy
             Some estimates suggest a 1:1000 write-to-read ratio for Twitter.
Consistency: An effective technique is needed to offer rapid feedback to the user (who liked someone's post),
             then to other specified users in the same region, and finally to all worldwide users linked to the Tweet.

Sequencers generate the unique IDs for the Tweets.
Sharded counters help to handle the count of multiple features such as viewing, liking and Retweeting of the accounts with millions of followers.
Pub-sub is used for real-time processing such as elimination of redundant data, organizing data, and much more.

CDN: When users search for a specified term or tag, the system first searches in the CDN proxy servers containing the most frequently requested content.
Twitter uses the Snowflake service to generate unique IDs for Tweets.
next_token: It is used to get the next page of results. The last result (page) will not have a next_token.

The decision to send specified promoted ads is made on the user_location parameter.
   For example, the user belongs to the NewYork city, so most probably it gets promoted ads associated or originated in that region.

Retweet a Tweet: a registered user Retweets (re-posts) someone's Tweet on Twitter
Ad-hoc clusters (occasional analysis) and cold storage clusters (less accessed and less frequently used data)
heavy read/write traffic (millions of QPS)
built the Blobstore storage system to store photos attached to Tweets. Now, it also stores videos, binary files, and other objects.
After a specified period, the server checkpoints the in-memory data to the Blobstore as durable storage.

uses MySQL and PostgreSQL, where it needs strong consistency, ads exchange, and managing ads campaigns.
After aggregation, the results are stored for ad-hoc analysis to BigQuery (data warehouse)

uses Apache Lucene for real-time search, which uses an inverted index.
Twitter stores a real-time index (recent Tweets during the past week) in RAM for low latency and quick updates.
The full index is a hundred times larger than the real-time index. However, Twitter performs batch processing for the full indexes.
stores this relationship in the form of a graph. Twitter used FlockDB, a graph database tuned for huge adjacency lists, rapid reads and writes, along with graph-traversal operations.

Caching is mainly utilized for storage (heavy read traffic), computation (real-time stream processing and machine learning),
   and transient data (rate limiters).
Memcached / Redis

Observability: Alert / Monitor / Configuration / Prediction
Zipkin, a distributed tracing system

ZooKeeper: configuration information, distributed synchronization, naming registry
    - Most real-time applications use ZooKeeper to store critical data.
    - It can also provide multiple services such as distributed locking and leader election in the distribution system.
    - Twitter uses ZooKeeper to store service registry, Manhattan clusters' topology information, metadata, and so on.
    - Twitter also uses it for the leader election on the various systems.

Real-world complex problems: heavy hitter problem
    - need multiple distributed counters to manage burst write requests (increments or decrements) against various interactions on celebrities' Tweets.
    - Each counter has several shards working on different computational units.
    - These distributed counters are known as sharded counters.

Trends(Top-k problem): Twitter shows Top-k trends (hashtags or keywords) locally and globally.
    - Hashtags with the maximum frequency (counts) become trends both locally and globally.
    - shows various promoted trends (known as 'paid trends') in specified regions under trends.

Timeline: Twitter shows two types of timelines: home and user timelines.
    - Sharded counters solve the discussed problems efficiently.
    - place shards of the specified counter near the user to reduce latency and increase overall performance like CDN
    - The nearest servers managing various shards of the respective counters are continuously updating the like or view counts with short refresh intervals.
    - We should note, however, that the near real-time counts will update on the Tweets with a long refresh interval.
    - The reason is the application server waits for multiple counts submitted by the various servers placed in different regions.

real-time processing, such as pulling Tweets, user interactions data, and many other metrics from the real-time streams and client logs, is achieved in the Apache Kafka.
Timeline service: sends a home timeline request
                  request is forwarded to the nearest CDN containing static data. If the requested data is not found, it's sent to the server providing timeline services.
Search service: first looks into the RAM in Apache Lucene to get real-time Tweets (Tweets that have been published recently).
                Then, this server looks up in the index server and finds all Tweets that contain the requested keyword(s).
                Next, it considers multiple factors, such as time, or location, to rank the discovered Tweets.
                In the end, it returns the top Tweets.
------------------------------------------------------------------------------------------------------------------------
Newsfeed - A recommendation system to ensure ranking and feed suggestions

A newsfeed of any social media platform (Twitter, Facebook, Instagram) is a list of stories generated by entities that a user follows
The challenging task is to provide a personalized newsfeed in real-time while keeping the system scalable and highly available.
The challenge here is that there is potentially a huge amount of content.
ranking service / ranking mechanism / support the ever-increasing number of users
The system can compromise strong consistency for availability and fault tolerance
partition tolerance (system availability in the events of network failure between the system's components) is necessary

Users' metadata storage
An entity could be a page, group, friends, and followers of a user.

High-level design of a newsfeed system:
    - Feed generation: The newsfeed is generated by aggregating friends' and followers' posts (or feed items) based on some ranking mechanism.
    - Feed publishing: When a feed is published, the relevant data is written into the cache and database. This data could be textual or any media content.
                       A post containing the data from friends and followers is populated to a user's newsfeed.

Web servers: The web servers encapsulate the back-end services and work as an intermediate layer between users and various services.
             Apart from enforcing authentication and rate-limiting, web servers are responsible to redirect traffic to other back-end services.
Notification service: sends a push notification
Newsfeed generation service: This service generates newsfeeds and keeps them in the newsfeed cache.
   - Calls the newsfeed generation service to generate feeds because some users don't often visit the platform, so their feeds are generated on their request.
   - It fetches the pre-generated newsfeed for active users who visit the platform frequently.
Newsfeed publishing service: This service is responsible for publishing newsfeeds to a users' timeline from the newsfeed cache.
                             It also appends a thumbnail of the media content from the blob storage and its link to the newsfeed intended for a user.

use a graph database to store relationships between users, friends, and followers.
property graph model: a graph database consisting of two relational tables:
    - For vertices that represent users
    - For edges that denotes relationships among them
vertex (user) or edge (relationship)

A memory-efficient way would be to store just the mapping between users and their corresponding posts in a table in the cache, that is., < Post_ID, User_ID>.
   During the feed publishing phase, the system will retrieve posts from the post database and generate the newsfeed for a user who follows another user with User_ID.

The newsfeed publishing service fetches a list of post IDs from the newsfeed cache.
   The data fetched from the newsfeed cache is a tuple of post and user IDs, that is., <Post_ID, User_ID>.
   Therefore, the complete data about posts and users are retrieved from the users and posts cache to create an entirely constructed newsfeed.

the fully constructed newsfeed is sent to the client (Alice) using one of the fan-out approaches.
   The popular newsfeed and media content are also stored in CDN for fast retrieval.

fan-out approaches (Fanout is the process of sending a newsfeed to all of the relevant friends/followers):
   - Pull model (fanout-on-load): Users can get data from the server manually whenever they require it
      > Users may not see new data until they submit a pull request.
      > Most pull requests will return an empty response if no new data is available, which may cause wasting resources.
        Therefore, it is difficult to decide when we should pull the data.
   - Push model (fanout-on-write): The benefit is that the users don't have to go through their friend's lists to acquire newsfeeds
      > the potential drawback of this approach is that the server may have to send millions of push updates to followers of a celebrity user.
   - Hybrid model:
      > halt pushing posts from users with a large number of followers (celebrities) and just send newsfeeds to people with a lower number of followers
      > allow the followers of celebrities to pull the updates.
      > Another option is to limit the push operations to only the online followers after a user publishes a post.

The updated newsfeed is delivered to the corresponding users on the next page/screen refresh event.
Eliminate posts including misinformation or clickbait from the candidate posts.
