building blocks
by some order of magnitude / increases by a factor of ten or more / linearly scale with increasing demands
use off-the-shelf components
strict consistency / strict consistency / durability target / privacy and regulatory requirements
handle ten times more load / be bogged down

Remote procedure calls (RPCs): an interprocess communication protocol / RPC spans the transport and application layers
The responsibilities of RPC runtime also include retransmission, acknowledgment, and encryption
The RPC method is similar to calling a local procedure, except that the called procedure is usually executed in a different process and on a different computer.
------------------------------------------------------------------------------------------------------------------------
Consistency Models:
    Eventual consistency (Weakest consistency) -> Causal consistency -> Sequential consistency -> Strict consistency / linearizability (Strongest consistency)

1. Eventual consistency ensures high availability
2. Causal consistency works by categorizing operations into dependent operations (causally-related operations) and independent operations / cause-and-effect relationship
3. Sequential consistency preserves the ordering specified by each client's program.
4. Strict consistency is the strongest consistency model. synchronous replication is one of the ingredients for achieving strong consistency, though it in itself is not sufficient. We might need consensus algorithms such as Paxos and Raft to achieve strong consistency.
   Linearizability affects the system's availability, which is why it's not always used. Applications with strong consistency requirements use techniques like quorum-based replication to increase the system's availability.

ACID: Atomicity, Consistency, Isolation, and Durability
CAP theorem: Consistency, Availability, and Partition Tolerance
------------------------------------------------------------------------------------------------------------------------
Failure Models:
Fail-stop (Easy to deal with) -> Crash -> Omission failures -> Temporal failures -> Byzantine failures  (Hard to deal with)

Fault tolerance techniques:
    Replication: One of the most widely-used techniques is replication-based fault tolerance
    Checkpointing: a technique that saves the system's state in stable storage when the system state is consistent
                   When a failure occurs in the system, we can get the last computed data from the previous checkpoint and start working from there.

Application servers primarily provide dynamic content, whereas web servers mostly serve static content to the client, which is mostly a web browser.
------------------------------------------------------------------------------------------------------------------------
commodity servers
a coarse-grained estimation / reasonable upper bounds
incoming data / outgoing data
bandwidth is measured in bits per second

To mitigate this issue, each cached record comes with an expiration time called time-to-live (TTL)

Algorithms of load balancers
    - Round-robin scheduling: a repeating sequential manner
    - Weighted round-robin
    - Least connections
    - Least response time
    - IP hash
    - URL hash
------------------------------------------------------------------------------------------------------------------------
anomalies (like dirty reads, dirty writes, read skew, lost updates, write skew, and phantom reads)
semi-structured and unstructured data / document databases are structureless

NoSQL:
    - Key-value database: Amazon DynamoDB, Redis, and Memcached DB
                          Primary key = Partition key + Sort key
    - Document database: MongoDB and Google Cloud Firestore
                         XML, JSON, BSON / composed of a hierarchical tree data structure
    - Graph database: Neo4J, OrientDB, and InfiniteGraph
    - Columnar database: Cassandra, HBase, Hypertable, and Amazon SimpleDB
                         store data in columns instead of rows
                         efficient for a large number of aggregation and data analytics queries

Drawbacks of NoSQL databases:
    - Lack of standardization
    - Consistency
      We won't have strong data integrity, like primary and referential integrity in a relational database
      Data might not be strongly consistent but slowly converging using a weak model like eventual consistency

Replication:
   - Synchronous replication
   - Asynchronous replication
Data replication models:
   - Single leader or primary-secondary replication
         Primary-secondary replication methods
             > Statement-based replication
             > Write-ahead log (WAL) shipping
             > Logical (row-based) log replication
   - Multi-leader replication
         Handle conflicts
             > Conflict avoidance
             > Last-write-wins: clock synchronization / clock skew
             > Custom logic
   - Peer-to-peer or leaderless replication
        Quorums: w + r > n

Data partitioning (or sharding)
   - Vertical sharding
        > used to increase the speed of data retrieval from a table consisting of columns with very wide text or a binary large object (blob)
   - Horizontal sharding
        > divide a table into multiple tables by splitting data row-wise
        > suitable to automate even under dynamic conditions
            - Key-range based sharding: The data routing logic uses the partition key at the application tier to map queries specified for a database shard
                Advantages:
                    > range-query-based scheme is easy to implement.
                    > Range queries can be performed using the partitioning keys, and those can be kept in partitions in sorted order.
                Disadvantages:
                    > Range queries can't be performed using keys other than the partitioning key.
                    > If keys aren't selected properly, some nodes may have to store more data due to an uneven distribution of the traffic.
            - Hash based sharding
                Advantages:
                    > Keys are uniformly distributed across the nodes.
                Disadvantages:
                    > We can't perform range queries with this technique. Keys will be spread over all partitions.
Consistent hashing
   - Advantages:
      > It's easy to scale horizontally.
      > It increases the throughput and improves the latency of the application.
   - Disadvantages:
      > Randomly assigning nodes in the ring may cause non-uniform distribution.
Re-balance the partitions
   - Avoid hash mod n
   - Fixed number of partitions:
      > create a higher number of partitions than the nodes
      > very important to choose the right number of partitions
      > used in Elasticsearch, Riak
   - Dynamic partitioning
      > when the size of a partition reaches the threshold, it’s split equally into two partitions
      > used in HBase and MongoD
   - Partition proportionally to nodes
      > the number of partitions is proportionate to the number of nodes, which means every node has fixed partitions
      > used by Cassandra and Ketama
Partitioning and secondary indexes
   Secondary indexes are the records that aren't identified by primary keys but are just a way of searching for some value.
      - Partition secondary indexes by document: local index
      - Partition secondary indexes by the term: global index
Request routing
   service discovery

ZooKeeper: a separate management server
Whenever there's a change in the partitioning, or a node is added or removed, ZooKeeper gets updated and notifies the routing tier about the change.
HBase, Kafka and SolrCloud use ZooKeeper

disadvantages of a distributed database:
    - data is required from multiple sites, which takes more time than expected
    - operations such as joins need to reconstruct complete relations by carefully fetching data
    - difficult to maintain consistency of data across sites
    - Updates and backups in distributed databases take time to synchronize data
hotspots
------------------------------------------------------------------------------------------------------------------------
hardware failures and network congestion

Content consistency in CDN:
   - Periodic polling: time-to-refresh (TTR)
   - Time-to-live (TTL)
   - Leases: lease denotes the time interval for which the origin server agrees to notify the proxy server if there’s any change in the data
             lease renewal / expiration of the lease /  lease duration (adaptive lease)
------------------------------------------------------------------------------------------------------------------------
Sequencer / globally unique IDs
we need a unique ID generator that acts as a primary key in a distributed setting—for example, a horizontally-sharded table.

First solution: UUID: 128-bit number
                    Cons: Using 128-bit numbers as primary keys makes the primary-key indexing slower, which results in slow inserts.
                          A workaround might be to interpret an ID as a hex string instead of a number.
                          might not be monotonically increasing
                          can't claim UUID to be deterministically unique / a chance of duplication
Second solution: using a database: auto-increment feature / modify the conventional auto-increment feature that increments by one to m
                                   difficult to scale for multiple data centers
Third solution: using a range handler
                    Pros: scalable, available, and yields user IDs that have no duplicates.
                          maintain this range in 64 bits, which is numeric.
                    Cons: lose a significant range when a server dies

two types of physical clocks available in a computer:
    - the time-of-day clock
       > lower resolution in comparison to monotonic counters
       > Network Time Protocol (NTP) can move the clock forward or backward, so it's not always monotonic.
       > It may or may not incorporate leap seconds
    - monotonic counters
       > usually have higher resolution than time-of-day clocks
       > should be used for the duration between two events rather than for the time.
       > These aren't meaningful across different nodes.
       > NTP might adjust it without violating monotonicity.
       > NTP can only speed up or slow down the counter rate of change by up to 0.05%.

Physical clocks drift over time due to many reasons: Temperature differences / The equipment's age / Manufacturing defects / Virtualized clocks
physical clocks are unreliable. For such clocks, the error can be 17 seconds per day. In a distributed environment, the clocks won't remain synced.
no matter how often we synchronize these clocks with each other or other clocks with accurate measurement methods, there will always be skew between the various clocks involved in a distributed system.

The Network Time Protocol (NTP) is a networking protocol for clock synchronization between computer systems over packet-switched, variable-latency data networks.
NTP intends to synchronize all participating computers within a few milliseconds of Coordinated Universal Time (UTC). It mitigates the effects of variable network latency.

4th solution: Use UNIX time stamps
    - Pros: simple, scalable, and easy to implement. It also enables multiple servers to handle concurrent requests
    - Cons: For two concurrent events, the same time stamp is returned and the same ID can be assigned to them. This way, the IDs are no longer unique.
5th solution: Twitter Snowflake
    - Pros: Twitter Snowflake uses the time stamp as the first component. Therefore, they’re time sortable. The ID generator is highly available as well.
    - Cons: IDs generated in a dead period are a problem.
            Another weak point of this system is its reliance on time
6th solution: Using logical clocks (Lamport and vector clocks) that need monotonically increasing identifiers for events.
    - Lamport clocks
        >  Cons: can't simply compare two clock values on any server to infer happened-before relationship
    - Vector clocks
        > Pros: maintain causal history—that is, all information about the happened-before relationships of events
7th solution: TrueTime API: Google’s TrueTime API in Spanner / reports an interval of time
                            Google deploys a GPS receiver or atomic clock in each data center, and clocks are synchronized within about 7 ms
    - Pros: We're able to generate a globally unique 64-bit identifier. The causality of events is maintained. The approach is scalable and highly available.
    - Cons: If two intervals overlap, then we’re unsure in what order A and B occurred.
            Spanner is expensive because it ensures high database consistency
------------------------------------------------------------------------------------------------------------------------
Data versioning

version history / requires a reconciliation effort

To handle inconsistency, we need to maintain causality between the events:
    - using the timestamps and update all conflicting values with the value of the latest request
      But time isn’t reliable in a distributed system, so we can’t use it as a deciding factor
    - Another approach is by using vector clocks.
      A vector clock is a list of (node, counter) pairs. There's a single vector clock for every version of an object.
      If two objects have different vector clocks, we're able to tell whether they're causally related or not.
      > This process of resolving conflicts is comparable to how it’s done in Git

network partitions or multiple server failures
Clock Truncation strategy to store a timestamp with each (node, counter) pair to show when the data item was last updated by the node.
                             Vector clock pairs are purged when the number of (node, counter) pairs exceeds a predetermined threshold
                             Because the descendant linkages can't be precisely calculated, this truncation approach can lead to a lack of efficiency in reconciliation.

configurable service / coordinator
Two ways for a client to select a node:
    - route the request to a generic load balancer.
    - use a partition-aware client library that routes requests directly to the appropriate coordinator nodes.

use a consistency protocol similar to those used in quorum systems (r + w > n)

Handle temporary failures:
    - easily affected by the network outage -> use a sloppy quorum instead of strict quorum membership
                                               In the sloppy quorum, the first n healthy nodes from the preference list handle all read and write operations.
    - Hinted Handoff: ensure that reads and writes are fulfilled if a node faces temporary failure.

Handle permanent failures:
    - The Merkle tree is a mechanism to implement anti-entropy, which means to keep all the data consistent.
      It reduces data transmission for synchronization and the number of discs accessed during the anti-entropy process.
         >  There's no need for synchronization if, for example, the hash values of two trees' roots are the same and their leaf nodes are also the same.

Promote membership in the ring to detect failures
    - Planned commissioning and decommissioning of nodes results in membership changes.
    - A gossip-based protocol also maintains an eventually consistent view of membership.
------------------------------------------------------------------------------------------------------------------------
Blob store is a storage solution for unstructured data.
blob (binary large object)
write once, read many (WORM)
Google Cloud Storage / Amazon S3 or Amazon Simple Storage Service / Facebook's Tectonic Filesystem
blob store is a read-intensive store
access privileges
a backup or shadow server in place of a manager node
checkpointing: take snapshots of the data at different time intervals

These are the two levels of replication:
    - Synchronous replication within a storage cluster.
    - Asynchronous replication across data centers and regions.
------------------------------------------------------------------------------------------------------------------------
throughput (megabits per second) and latency (round-trip time)
We embed logging or monitoring code in our applications, called Code Instrumentation, to collect information of interest

drawbacks of  monitoring tool
   - sends its metrics to the monitoring system, resulting in a heavy traffic load on the infrastructure
   - We must also install daemons on each of these targets to send metrics to the monitoring server, which requires additional work.

Pull-based approach avoids overloading the network traffic by fetching the data itself.

Restrain the log size:
   - sampling: use a sampler service that only logs a smaller set of messages from a larger chunk / selectively pick a representative data set
   - categorization: production logs are set to print messages with the severity of WARNING, ERROR and FATAL/CRITICAL

Logging is an I/O-intensive operation
Cold storages are low-cost storages where archived data is kept.
Correctly ordering the log in a chronological (or causal) order simplifies log analyses.
------------------------------------------------------------------------------------------------------------------------
Bandwidth estimation: Incoming traffic / Outgoing traffic
Indexing in a Distributed Search
    - fuzzy search: approximate string matching rather than exact matching
    - An inverted index is a HashMap-like data structure that employs a document-term matrix
      terms: Frequently occurring words / document-term matrix maintains a term-level index
         > Advantages of using an inverted index
             - facilitates full-text searches
         > Disadvantages of using an inverted index
             - storage overhead
             - Maintenance costs (processing) on adding, updating, or deleting a document

search engine optimization (SEO) schemes

offline phase involves data crawling and indexing in which the user has to do nothing
online phase consists of searching for results against the search query by the use

The two most common techniques used for data partitioning in distributed indexing are these below:
    - Document partitioning
    - Term partitioning

periodic heartbeats / colocation (both searching and indexing are performed on the same node)
Parallel indexing and searching, where both of these processes are colocated on the same nodes.

Replication factor and replica distribution
    - Generally, a replication factor of three is enough
    - One of the three nodes becomes the primary node, while the other two are replicas.

downsides / resource-intensive operations
MapReduce framework: Cluster manager / Mappers / Reducers
------------------------------------------------------------------------------------------------------------------------
A cache is a temporary data storage that can serve data faster by keeping data entries in memory.
cache hit / cache miss

A distributed cache is a caching system where multiple cache servers coordinate to store frequently accessed data
Caches use the locality of reference principle

Writing policies:
    - Write-through cache: ensures strong consistency between the database and the cache
    - Write-back cache: have small writing latency
    - Write-around cache: such a strategy isn’t favorable for reading recently updated data
Eviction policies: replace less frequently accessed data with most frequently accessed data
    - Least recently used (LRU)
    - Most recently used (MRU)
    - Least frequently used (LFU)
    - Most frequently used (MFU)
Cache invalidation: Certain cached data may get outdated / remove stale or outdated entries from the cache / time-to-live (TTL) value
    - Active expiration
    - Passive expiration
Storage mechanism
    - Hash function
        > Identify the cache server in a distributed cache to store and retrieve data.
        > Locate cache entries inside each cache server.
    - doubly linked list
        > bloom filters
    - dedicated cache servers
        > There's flexibility in terms of hardware choices for each functionality.
        > It’s possible to scale web/application servers and cache servers separately.
    - Co-located cache
        > the reduction in CAPEX and OPEX of extra hardware
Cache client

Maintain cache servers list
    - Solution 1: have a configuration file in each of the service hosts
                  configuration service can be updated through a push service by any DevOps tool
                  have to be manually updated and deployed through some DevOps tools
    - Solution 2: store the configuration file in a centralized location
                  still need to manually update the configuration file and monitor the health of each server
    - Solution 3: An automatic way of handling the issue is to use a configuration service that continuously monitors the health of the cache servers.
                  In addition to that, the cache clients will get notified when a new cache server is added to the cluster.
                  Finally, the cache clients obtain the list of cache servers from the configuration service.

We used consistent hashing. Finding a key under this algorithm requires a time complexity of O(log(N)), where N represents the number of cache shards.
The LRU eviction approach uses a constant time to access and update cache entries in a doubly linked list.
Adding replicas reduces the load on hot shards. Another way to handle the hotkeys problem is to do further sharding within the range of those keys.

But synchronous writing for strong consistency in different data centers has a serious performance implication that isn’t welcomed in caching systems.
    We usually use asynchronous replication across data centers.

In the case of caching, the asynchronous mode is favored for improved performance.
   Consequently, our caching system suffers from inconsistencies.
   Alternatively, strong consistency comes from synchronous writing,
   but this increases the overall latency, and the performance takes a hit.

locking mechanisms: A mechanism by which access to shared data is limited to a single thread only. Thus achieving synchronization.
   - Limited locking: In this strategy, only specific sections of the entire data structure will be locked.
   - Offline eviction: only the required changes will be recorded while performing different operations until it’s necessary to commit the changes
   - Lock-free implementation

Memcached versus Redis
Memcached:
   - Both the key and the value are strings
   - each server follows the shared-nothing architecture (servers are unaware of each other, and there's no synchronization, data sharing, and communication between the servers)
   - achieve almost a deterministic query speed (O(1)) serving millions of keys per second

Redis:
   - can be used as a cache, database, and message broker
   - built-in replication mechanism, automatic failover, and different levels of persistence
   - doesn't provide strong consistency due to the use of asynchronous replication.
   - built-in cluster support: Redis Sentinel
   - Redis client-server communication with pipelining: Pipelining is the process of combining multiple requests from the client side without waiting for a response from the server.

------------------------------------------------------------------------------------------------------------------------
RESHADED Approach for System Design
Requirements / Estimation / Storage schema (optional) / High-level design / API design / Detailed design / Evaluation / Distinctive component/feature
------------------------------------------------------------------------------------------------------------------------









